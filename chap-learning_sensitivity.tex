\chapter{Learning uncertainty tubes via recurrent neural networks} \label{chap:NN}
\markboth{Learning uncertainty tubes via recurrent neural networks}{}% To set left/right header
% \localtableofcontents

This chapter introduces the second major contribution of this thesis: a deep learning approach for rapidly and accurately estimating sensitivity-based uncertainty tubes. 
As demonstrated in Chapter~\ref{chap:samp}, the computation of such tubes using \myglsentry{odes} considerably reduces the efficiency of sampling-based methods, particularly for complex systems.
To overcome this limitation, this chapter introduces a method based on \gls{rnn} to eliminate the dependency on solving \myglsentry{odes}.
By leveraging structural similarities between \myglsentry{odes} and \myglsentry{rnn}, the proposed neural network architecture achieves significantly faster predictions of uncertainty tubes compared to traditional solvers, thereby improving computational efficiency while maintaining accuracy.

This chapter is organized as follows: Section~\ref{sec:learning_overview} begins with an overview of parallelization techniques for \myglsentry{odes} and learning methods for \myglsentry{odes}.
Section~\ref{sec:method} details the proposed method, including the neural network architecture and dataset generation process. 
Evaluation metrics, training procedures, and implementation details are discussed in Section~\ref{sec:nn_eval}. 
Finally, Section~\ref{sec:nn_results} presents the results for the quadrotor and differential drive robot, while Section~\ref{sec:concl} concludes the chapter.

This chapter is associated to the following publication in ECAI 2024: Wasiela, S., Bouhsain, S. A., Cognetti, M., Cortés, J., and Simeon, T. (2024, October). "Learned Uncertainty Tubes via Recurrent Neural Networks for Planning Robust Robot Motions." In 27th European Conference on Artificial Intelligence (ECAI) (pp. 4385-4392).\footnote{The results presented in this chapter slightly differ from those in the article due to improvements in implementation.}

% Parallelization of ODE (Ordinary Differential Equations) solvers for systems like a quadrotor is indeed possible, but it comes with certain challenges due to the inherent sequential nature of ODE solvers. Each time step generally depends on the previous one, making it difficult to parallelize directly. However, various techniques and strategies can help in parallelizing ODEs, even for small time steps. Let's explore this in more detail.
% 1. Parallelization in ODE Solvers

% ODE solvers like Runge-Kutta, Euler, or Adams-Bashforth typically compute solutions in a time-stepping manner, where each step depends on the solution from the previous step. This sequential dependency makes it hard to directly parallelize time integration.

% However, some strategies can parallelize parts of the problem:
% 1.1 Parallelizing the Evaluation of Derivatives

% The right-hand side (RHS) of the ODE system (i.e., the function that describes the system's dynamics) can often be parallelized if it involves multiple independent computations. For example, in a quadrotor, if the dynamics involve complex matrix operations, vector algebra, or sensor models, these computations can be parallelized across multiple cores or even on a GPU.
% 1.2 Domain Decomposition for Parallelization

% This approach involves splitting the physical domain or the variables of the system and solving them in parallel. For a quadrotor, domain decomposition may not apply as easily as it would for a large physical domain problem (like fluid dynamics), but it could still be used if the quadrotor's dynamics are complex and involve interacting subsystems. For instance, rotor dynamics, attitude control, and position dynamics could potentially be solved in parallel if they are weakly coupled.
% 1.3 Multiple Shooting or Parareal Algorithms

% These methods introduce parallelization by splitting the time domain into multiple intervals and solving the ODE in parallel on each subinterval.

%     Parareal Algorithm: This method allows for parallel time integration by solving coarse approximations over large time intervals in parallel, followed by corrections using fine time steps. While the fine steps remain sequential, the coarse steps can be parallelized. The Parareal method is well-suited for large-scale systems but might be overkill for small systems like a quadrotor, especially with small time steps.

%     Multiple Shooting: In this technique, the time domain is divided into smaller intervals, and the ODE is solved independently in each interval. The solutions are then stitched together using boundary conditions. This method is typically used for stiff ODE problems or optimization-based control (like Model Predictive Control).

% 1.4 Parallel Integration of Different States

% In some cases, the state variables of a system might evolve independently or with weak coupling, allowing for parallel integration of different states. For a quadrotor, some aspects of the dynamics (e.g., attitude and position) could potentially be integrated separately in parallel, especially if the time scales are slightly different.
% 2. Time-Stepping Constraints and Relaxation

% You mentioned time-step constraints. In ODE solvers, especially explicit solvers like Runge-Kutta or Euler, the time step size is usually limited by stability constraints (e.g., Courant-Friedrichs-Lewy (CFL) condition for PDEs or step size for stiff ODEs).

%     Relaxing Time-Step Constraints: Parallelization techniques like Parareal introduce time-step relaxation by allowing coarse-grain parallel computations over larger time intervals, then correcting with smaller steps sequentially. This is useful for accelerating convergence in large-scale simulations with longer time horizons.

%     Using Small Time Steps in Parallelization: If your goal is to maintain small time steps (e.g., for accuracy, stability, or real-time control in a quadrotor), relaxation-based methods might not be as helpful. However, parallelization can still help in other ways:
%         Task-based Parallelism: Use parallel computing resources to speed up the evaluation of forces, torques, control inputs, sensor models, or optimization procedures (if using real-time control).
%         Predictor-Corrector Methods: These methods can use a fast, coarse-grained predictor step, followed by parallel corrections that refine the solution. The corrections can be done in parallel with small time steps, making it more suitable for small time steps.
%         Parallelization of Subsystems: Parallelizing distinct components of the quadrotor's dynamics (e.g., thrust dynamics, rotational dynamics, or environmental interactions) can improve computational efficiency while keeping the same small time steps.

% 3. Maintaining Small Time Steps in Parallelization

% To directly answer your question, you can use parallelization techniques while keeping small time steps, but the degree of parallelization might be more limited compared to approaches where time steps are relaxed. The most promising approaches in this scenario would be:

%     Parallel evaluation of the system's dynamics: Evaluating the dynamics (i.e., the forces and moments) in parallel could reduce computation time significantly, especially if your quadrotor model involves complex dynamics (e.g., aerodynamic drag, rotor dynamics, etc.).

%     Task-level parallelism: In real-time control applications, parallelizing sensor fusion, state estimation, and control computations (while maintaining small time steps) can improve performance.

%     Multiple shooting methods: Even with small time steps, the multiple shooting method can potentially parallelize certain segments of the time domain, but you'd need to stitch together the results carefully, ensuring stability.

% Conclusion

% While parallelization techniques like Parareal or Multiple Shooting often involve relaxing time-step constraints, it is still possible to parallelize certain parts of the quadrotor's ODE solver without relaxing the small time-step constraints. You can achieve this by focusing on parallelizing the evaluation of the system's dynamics, using task-level parallelism, or exploiting weakly coupled subsystems. However, the benefit of parallelization will likely be more constrained compared to systems where time-step constraints are relaxed.

% 3. Challenges of Using Parareal for Control Stability

% The Parareal method works by first running a coarse-grained simulation with larger time steps, followed by corrections using finer time steps. If the coarse solver uses large time steps, it could violate the control stability requirements during the initial stages of the Parareal iterations. This would make it unsuitable for real-time control if the initial approximation (coarse solver) is far from accurate, as the control system relies on frequent updates.



% Your GRU (Gated Recurrent Unit) neural network might be faster than other methods for solving an ODE on a CPU for several reasons related to its structure and the way neural networks handle data. The efficiency of GRU in this context largely depends on the number of operations it performs and how well it's optimized for CPU computations. Let's break it down:
% Why GRU is Faster for ODE Solving on CPU:

%     Fewer Parameters: GRUs are known for their simplicity compared to other recurrent neural networks (RNNs) like LSTMs. GRUs have fewer gates and parameters, making them computationally cheaper.
%         GRUs have only two gates (reset and update), while LSTMs have three gates (input, forget, output), which translates to fewer matrix multiplications per time step.
%         Fewer parameters also mean less memory consumption, which reduces the computational load when training or running the model.

%     Parallelism: Neural networks, including GRUs, can be structured in such a way that they process multiple data points (or time steps) in parallel, especially during training or inference. This can make GRUs more efficient than traditional methods for solving ODEs, such as Runge-Kutta, which might be more sequential in nature.

%     Reduced Complexity of Time-Stepping:
%         Numerical methods for solving ODEs, such as Runge-Kutta or implicit methods, often involve multiple function evaluations at each time step. GRUs, however, learn a representation of the system's dynamics, effectively "learning" a function approximation, which can be faster at runtime.
%         GRUs can use a single pass per time step (or batch of time steps), while traditional solvers may require many steps or iterations at each point.


% Model Type	Inference Time	Remarks
% RNN (GRU, LSTM)	Faster	Efficient for discrete, regular time steps; fixed computation per time step
% Neural ODE	Slower	Inference depends on ODE solver complexity and system dynamics

% In most practical scenarios, RNNs (GRUs, LSTMs) will offer faster inference compared to Neural ODEs due to the absence of ODE solvers and the simpler, more direct nature of their computations. Neural ODEs are more suited for scenarios where continuous dynamics are the priority, but this often comes at the cost of slower inference times.

% Time Steps vs. ODE Solver Integration:

%     In RNNs, the number of time steps is fixed, and you perform one forward pass per time step.
%     In Neural ODEs, the time steps are determined by the ODE solver, which can be adaptive. The solver may take many small steps or fewer large steps, depending on the system's complexity.

%     RNNs (GRU, LSTM, etc.):

%     RNNs are designed to handle discrete time sequences. They take input in the form of a time series with fixed time steps (e.g., t1,t2,t3,...) and learn to map these discrete states over time.
%     RNNs operate by passing hidden states from one time step to the next, effectively modeling the temporal correlations in the data.
%     LSTMs and GRUs are specific variants of RNNs that improve on the basic RNN structure by addressing the problem of vanishing/exploding gradients in long sequences. They do this using gating mechanisms that control the flow of information through time.
%     Since RNNs work with fixed time steps, they model time as discrete, and handling continuous dynamics requires fine-tuning, such as choosing a time step size.

% Neural ODEs:

%     Neural ODEs, on the other hand, model continuous-time dynamics. They replace the discrete sequence of states (as in RNNs) with a differential equation for the hidden states:
%     dh(t)dt=f(h(t),t,θ)
%     dtdh(t)=f(h(t),t,θ) where ff is a neural network and θθ are its parameters. The dynamics evolve continuously over time, and you can evaluate the state at any arbitrary time tt, not just at predefined discrete steps.
%     Neural ODEs use ODE solvers to numerically integrate the system over time, allowing for flexible handling of varying time steps and even irregular or non-uniform time intervals.


% The use of a simple GRU architecture instead of Neural ODEs or Physics-Informed Neural Networks (PINNs) to approximate ODE solutions in the context of the paper by Wasiela et al. likely stems from several practical advantages that GRUs offer, particularly in the domain of real-time robot motion planning and control. Here are the key reasons and advantages of this choice:
%     1. Efficiency and Simplicity in Real-Time Applications
    
%     GRUs are known for their computational efficiency, especially in real-time applications like robotics where fast decision-making is crucial. A GRU-based model can be trained relatively easily and deployed quickly for inference, allowing the robot to plan and adjust its motions on the fly.
    
%         Fewer Parameters: Compared to models like Neural ODEs or more complex recurrent architectures, GRUs have a simpler structure and fewer parameters, making them faster and more efficient during inference. This makes GRUs particularly well-suited for tasks where real-time performance is required, such as robot motion planning.
    
%     2. Handling Sequential Data with Temporal Dependencies
    
%     GRUs are designed to handle sequential data and are efficient at modeling temporal dependencies, which are common in dynamical systems. In the context of approximating ODEs for robot motion, where states evolve over time, GRUs can track the evolving state of the robot and make predictions about future states efficiently.
    
%         Unlike Neural ODEs, which require solving differential equations iteratively through a solver, GRUs can directly approximate the state transitions (i.e., how the system evolves in time) in a single forward pass per time step, making them faster for inference.
    
%     3. Avoiding the Complexity of ODE Solvers
    
%     Neural ODEs, while powerful, often rely on the use of ODE solvers during training and inference, which can be computationally expensive and require adaptive step sizes. These solvers often need to balance accuracy with computational speed, which can become a bottleneck in real-time applications.
    
%         GRUs, in contrast, do not require iterative solvers to propagate forward in time. Once trained, a GRU can generate the next state in the sequence in a single step, leading to much faster performance, especially in resource-constrained environments like robotics.
    
%     4. Flexibility and Generalization
    
%     GRUs are flexible in terms of what kind of sequential data they can handle. Unlike Physics-Informed Neural Networks (PINNs), which integrate physical constraints directly into the learning process (requiring knowledge of the system’s physics and explicit boundary conditions), GRUs can approximate ODEs without needing explicit knowledge of the underlying physical laws.
    
%         This makes GRUs a more flexible and practical choice when the goal is to approximate the system’s behavior based on empirical data, rather than being strictly bound to physical laws or differential equations. This is especially useful in real-world scenarios where the system dynamics may be complex or partially unknown.
    
%     5. Data-Driven Approach with Learned Dynamics
    
%     In applications where data-driven approaches are preferred (such as when the robot's environment is highly dynamic or uncertain), GRUs can learn to model the system’s dynamics based purely on input-output data without needing a detailed mathematical model.
    
%         Neural ODEs and PINNs require knowledge of the system’s governing equations (or their differential form), which may not always be available or easy to define in the context of robot motion planning with uncertainties. A GRU can capture complex non-linear dynamics without needing to explicitly model the differential equations.
    
%     6. Handling Uncertainty and Robustness
    
%     In the paper, the GRU is used to model Learned Uncertainty Tubes, which suggests that the authors are dealing with uncertain environments and need a way to account for this uncertainty in planning robust robot motions. GRUs are good at capturing uncertainty in sequential predictions because of their inherent ability to model and propagate information over time.
    
%         GRUs can be trained to model not just the system's trajectory, but also the potential deviations or uncertainties in future states. This allows the GRU-based model to handle uncertain environments more robustly and efficiently, which may be harder to achieve with Neural ODEs or PINNs due to the complexity of integrating uncertainty into those frameworks.
    
%     7. Empirical Success and Ease of Implementation
    
%     Another reason for choosing GRUs could simply be their empirical success in time-series forecasting and modeling dynamic systems. They are widely used and have proven to work well in practice. Additionally, GRUs are easier to implement and train compared to Neural ODEs or PINNs, which often require more sophisticated training setups, custom loss functions, and solvers.
%     Summary of Advantages of Using GRUs over Neural ODEs or PINNs:
    
%         Faster inference: GRUs don't need iterative ODE solvers, making them more efficient for real-time applications.
%         Simplicity: GRUs are simpler to implement and require fewer resources compared to Neural ODEs or PINNs.
%         Good at handling sequential data: GRUs excel at modeling time-series data and can approximate temporal dependencies effectively.
%         Data-driven and flexible: GRUs can approximate the dynamics of a system without needing a detailed mathematical model of the underlying physics.
%         Handling uncertainty: GRUs are well-suited for modeling uncertainty in robot motion planning, as they can track evolving dynamics and uncertainties over time.
    
%     By using GRUs, the authors likely balance between achieving real-time performance, modeling complex dynamics, and managing uncertainties—all essential for robust robot motion planning.

%     You're absolutely right. Since the authors in the mentioned paper want to bypass the ODEs and avoid calling traditional solvers (like Euler, Runge-Kutta, etc.), Neural ODEs (NODEs) are not suitable because NODEs inherently rely on ODE solvers during inference. Let's now consider whether Physics-Informed Neural Networks (PINNs) could be an alternative, given the situation.
%     Why PINNs Might Not Be the Right Fit in This Context
    
%     Even though PINNs integrate known ODEs into the learning process, there are several reasons why they might not be the best choice for this particular case where the authors want to completely bypass the ODEs:
    
%     2. PINNs May Not Be as Efficient in Real-Time
    
%         PINNs often require more computational resources and longer training times because they solve ODEs in a way that enforces the governing equations (through residuals and constraints). This can lead to slower inference times compared to using something like a GRU, which is purely data-driven and bypasses both the explicit and implicit use of ODEs.
    
%         In the context of robot motion planning—which requires fast, real-time predictions—PINNs might be overkill or too computationally demanding for the purpose, especially when the focus is on efficient approximation of motion without solving differential equations explicitly.
    
%     3. PINNs Are Better Suited When Physics Constraints Are Critical
    
%         PINNs excel when you want to incorporate physical constraints or laws explicitly during learning, particularly when data is sparse or noisy. However, in the case where the authors want to approximate the ODEs using a simple data-driven approach (like a GRU), it suggests that they are prioritizing speed and simplicity over strict adherence to the physics.
    
%         If the authors are confident that the GRU can capture the dynamics without relying on the exact physical laws during inference, then the use of PINNs—which are designed to enforce those physical laws—becomes unnecessary.

\section{ODEs parallelization and learning techniques}\label{sec:learning_overview}

This section begins with an overview of parallelization techniques for \myglsentry{odes}, such as the parareal algorithm~\cite{cParareal}, which aims to accelerate computations by distributing the workload across multiple processors. 
It then explores "standard" learning approaches related to \myglsentry{odes}, including \gls{pinns} and NeuralODEs, which incorporate the structure and dynamics of differential equations into the learning process. 
Finally, the section introduces \myglsentry{rnn}, emphasizing their structural similarities to \myglsentry{odes} and their potential for efficiently modeling temporal and sequential data in the context of dynamical systems.

\subsection{ODEs parallelization}

This section discusses various techniques used to parallelize the computation of \myglsentry{odes}, such as the parareal algorithm~\cite{cParareal}, which divides the solution process into multiple steps to be processed in parallel, improving efficiency.


While ODE parallelization methods can be useful for speeding up large-scale simulations or off-line computations (such as in scientific simulations), they are generally not well-suited for real-time control applications due to concerns about stability, synchronization, and numerical accuracy. In control systems, it's often more important to focus on stability-preserving numerical integration and real-time performance rather than trying to parallelize the solution of ODEs.


\subsection{Learning with embedded dynamic}

This section explores learning techniques that directly incorporate the structure of dynamical systems into the learning process, facilitating accurate predictions of system behavior.

\begin{itemize}
    \item NeuralODEs: A method that treats ODE solvers as neural networks to learn dynamic systems.
    Neural ODEs are useful when we don’t know the system dynamics and want a flexible, data-driven method to model them.
    \item PINNs: A learning framework that embeds the physics governing differential equations into the training process.
\end{itemize}

\subsection{Learning sequences of data}

This section discusses learning methods suited for modeling sequences of data, such as time series or trajectories, which are common in dynamical systems.

\begin{itemize}
    \item Transformers
    \item RNNs
\end{itemize}

\section{Method} \label{sec:method}

This section presents a multi-task learning neural network based on a GRU architecture, which takes as input the desired states of the dynamic system and outputs the nominal control inputs $\u_n$, as well as the uncertainty tubes around the states $\Rq$ and the control inputs $\Ru$. 

\subsection{Problem statement}

The goal is to train a neural network to approximate sensitivity-based uncertainty tubes, hence avoiding the computational cost of solving many \myglsentry{odes}. 
Moreover, the model aims at predicting the control inputs that the system will exert to successfully track a desired trajectory, in addition to the uncertainty tubes on these control inputs. 
Given a sequence of desired robot state vectors $\textbf{M} = \{\q_{d}^0, \q_{d}^1, ..., \q_{d}^n\}$ representing the desired robot motion (i.e. a desired trajectory to follow), the task at hand is to learn a function $\boldsymbol{g}$ that estimates the radii of uncertainty tubes for each state $\q_{d}^k$ as well as the robot control inputs at this state such that:
\begin{equation}\label{eq:prob_statement}
\{\Rq^{k}, \Ru^{k},\u^{k}\} = \boldsymbol{g}(\q_{d}^0, \q_{d}^1, ... \q_{d}^{k-1})
\end{equation}
Since evaluating the function $\boldsymbol{g}$ on $\q_{d}^k$ depends on all the previous states of the robot in $\textbf{M}$, recurrent neural network architectures are a good fit given their ability to encode and accumulate temporal information while keeping inference time low.

\subsection{Neural network architecture}\label{sec:architecture}

\begin{figure} [t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/learning_quadrotor/SensiNN_GRU.png}%
    \caption{Representation of the proposed neural network architecture.
    }%
    \label{fig: NN}%
\end{figure}

A representation of the neural network architecture is presented in Figure~\ref{fig: NN}.
Blue blocks refer to the inputs of the model which are composed of an initial hidden state $\boldsymbol{h}_{0}$ and of a sequence of desired robot states evaluated along a desired trajectory denoted $\boldsymbol{\q_{in}^k}$, where $k$ refers to the $k$-th state of the desired trajectory. 

The outputs of the neural network correspond to the green blocks.
It is composed of the predicted uncertainty tubes radii along the state $\Rq$ and of the uncertainty tubes radii associated with the control inputs of the system $\Ru$.
Moreover, the control input values $\u$ are required for the motion planning algorithm to perform robust feasibility checks, and their computation depends on \myglsentry{odes} forward integration (as shown in Section~\ref{sec:tubes}). 
Since the proposed approach aims at eliminating the need for solving \myglsentry{odes}, the neural network is also trained to predict the control inputs.
Finally, $\boldsymbol{h}_{N}$ corresponds to the hidden state at the last point of our sequence (i.e., the last desired trajectory state).

At $k=0$, the first state in the input sequence $\boldsymbol{\q_{in}^0}$ and the initial hidden state $\boldsymbol{h}_0$ are given to a single-layer GRU block, which outputs an updated hidden state $\boldsymbol{h}_1$. 
The latter is then fed to a multi-layer perceptron (MLP), each layer followed by a ReLU activation function except the final one, to obtain the predicted control inputs $\u^0$, the state uncertainty tubes $\Rq^0$ as well as the control uncertainty tubes $\Ru^0$. 
The updated hidden state $\boldsymbol{h}_1$ is then given back to the GRU block along with the second element of the input sequence. 
This process is repeated for each element $\boldsymbol{\q_{in}^k}$ until all predictions are obtained.

The network is intended to be used in a sampling-based tree planner (see Chapter~\ref{chap:sampNN}), where local trajectories are concatenated to form a global one. 
Thus, since the hidden state encodes and accumulates temporal information about the input sequence, the final hidden state $\boldsymbol{h}_{N}$ of a local trajectory obtained after an iteration of a sampling-based tree planner can be saved and then given back as the initial hidden state $\boldsymbol{h}_{0}$ for future local trajectories considered during the next iterations. 
Therefore, in practice, this initial hidden state is generally not null.

\subsubsection{Differential drive robot}\label{sec:unic_nn_architecture}

As the proposed model aims to bypass solving the \myglsentry{odes}, which starts by computing the appropriate control inputs, a reasonable initial guess is to start the prediction using the same inputs as those used by the controller.
The \myglsentry{dfl} controller inputs consist of the desired robot position, linear velocities, and accelerations, as shown in Equation~\ref{eq:unic_control_law}. 
However, to ensure that the learned model is independent of the workspace boundaries used during planning (i.e., the robot's position limits) and since accelerations are not considered for this robot in this thesis, only the desired linear velocities are retained as input components for the network.
Therefore, the input to the neural network for the differential drive robot case is a vector $\boldsymbol{\q_{in}^k} = [\boldsymbol{v_d}]^T \in \mathbb{R}^2$, where $k$ refers to the $k$-th state of the desired trajectory.

The predicted outputs are composed of the predicted uncertainty tubes radii along the $\{x,y\}$-axis of the state $\Rq=$[$r_{x},\,r_{y}]^T \in \mathbb{R}^2$, and the uncertainty tubes radii associated with the control inputs of the system $\Ru = [r_{u1},\,r_{u2}]^T \in \mathbb{R}^2$.
The control input values predicted by the neural network are $\u = [u_1 ,\, u_2]^T \in \mathbb{R}^2$, which correspond to the right and left wheel speeds.

\subsubsection{Quadrotor}\label{sec:quad_nn_architecture}

In the quadrotor case, the controller inputs consist of the desired robot position, linear velocities, accelerations, yaw angle orientation, and yaw angular velocities, as shown in Equation~\ref{eq:desftau}. 
However, in order to make the learned model independent of workspace boundaries used during planning (i.e. robot position and orientation bounds) and initial robot orientations, only the desired linear velocities, accelerations, and yaw angular velocities ($\Dot{\Psi}_d$), are kept as the network input components.
Hence, the input to the neural network for the quadrotor case is a vector $\boldsymbol{\q_{in}^k} = [\boldsymbol{v_d} ,\, \boldsymbol{a_d} ,\, \Dot{\Psi}_d]^T \in \mathbb{R}^7$, where $k$ refers to the $k$-th state of the desired trajectory.

The predicted outputs for the quadrotor case, are composed of the predicted uncertainty tubes radii along the $\{x,y,z\}$-axis of the state $\Rq=$[$r_{x},\,r_{y},\,r_{z}]^T \in \mathbb{R}^3$, and the uncertainty tubes radii associated with the control inputs of the system $\Ru = [r_{u1},\,r_{u2},\,r_{u3},\,r_{u4}]^T \in \mathbb{R}^4$.
The control input values predicted by the neural network are $\u = [u_1 ,\, u_2 ,\, u_3 ,\, u_4]^T \in \mathbb{R}^4$, which correspond to the squared propeller speeds.

\subsection{Dataset}\label{sec:dataset_general}

\begin{figure} [t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/learning_quadrotor/dataset_generation.png}%
    \caption{Dataset generation process. 
    Starting from the hovering state $\q_{init}$, at each iteration a new state $\boldsymbol{q_{rand}}$ is randomly sampled and connected to the previous state $\boldsymbol{q_{prev}}$ until a total trajectory length ($T_F$) of 15s is reached.
    Data annotation is performed by simulating the tracking of the generated trajectory under nominal parameters.
    }%
    \label{fig: data_generation}%
\end{figure}

In order to train the proposed neural network, a dataset of trajectories computed in an obstacle-free environment is generated, as depicted in Figure~\ref{fig: data_generation}.
This ensures that the resulting learned model is totally independent of the environment and only depends on the system.
As a result, it is highly efficient when used in a sampling-based motion planning context, where the same robot model can be applied across different environments.

Each global trajectory starts from the same initial state ($\q_{init}$) initialized at zero velocities and accelerations.
Note that this initialization does not hurt the generalizability of the model to different initial conditions as every dynamic state can be connected to the null state by mean of a local planner.
Based on the same principle as a sampling-based planner, the global trajectory is made up of local sub-trajectories until a total distance $D_{max}$ (according to the state space metric) is reached.
Each local sub-trajectory is generated by uniformly sampling an arrival state ($\q_{rand}$) and connecting it to the previous sampled state ($\q_{prev}$) using an appropriate local planner, based on the robot being considered.
If the local trajectory (between $\boldsymbol{q_{rand}}$ and $\boldsymbol{q_{prev}}$) distance (according to the state space metric) is greater than a maximum local distance $d_l$, it is truncated to $d_l$. 
This maximum local distance refers to the planner maximum local distance (see Section~\ref{sec:samp_simu}).
Similarly, when the total trajectory distance exceeds $D_{max}$ after adding a new state, it is truncate at $D_{max}$.

To generate the outputs, i.e. to annotate the data with uncertainty tubes and control inputs, the closed-loop dynamic and the sensitivity matrices are computed by simulating the tracking of the global trajectories using an integration time step $\Delta T$ which corresponds to the controller time step.
The data annotation was performed by computing and integrating the set of \myglsentry{odes} composed of Equation~(\ref{eq:dyna}), Equation~(\ref{eq:ctrl}), and Equation~(\ref{eq:dyna_sensi}) by mean of the Euler \myglsentry{odes} solver along a desired trajectory. 
Once $\bPi$ and $\bTheta$ had been computed, a simple projection was performed to recover the tubes thanks to Equation~(\ref{eq:radius}).
Note that the control inputs $\u$ are computed during the \myglsentry{odes} resolution.

Using this mechanism, a training set composed of 8.000 trajectories and a validation set composed of 2.000 trajectories were generated for both the quadrotor and the differential drive robots, making sure that every trajectory in the dataset is different.

\subsubsection{Differential Drive Robot}\label{sec:dataset_unic}

\begin{table}[t]
    \centering
    \begin{tabular}{ | c | c || c |}
    \hline
      \textbf{Output}  & \textbf{Validation set}  & \textbf{Test set} \\ \hline
    $\Rq$ & $1.0e^{-1} \pm 2.0e^{-2}$ & $1.1e^{-2} \pm 2.1e^{-2}$ \\ \hline
    $\u$ & $12469.3 \pm 861.6$ & $12476.8 \pm 1016.5$ \\ \hline
    $\Ru$ & $7782.6 \pm 3172.3$ & $7828.6 \pm 2845.7$ \\ \hline
\end{tabular}
\caption{
Mean and standard deviation of the output vector components norm after data annotation for the validation and test sets for the differential drive robot case.
$\Rq$ is expressed in \emph{m}, and $(\u, \Ru)$, are squared propeller angular velocities [(rad/s)].}
 \label{tab:datas_stats_unic}
\end{table}

The local planner used to generate the differential drive robot datasets is the Dubins method (see Section~\ref{sec:dubins}).
The trajectories in the dataset were generated with a maximum total distance $D_{max}$ of 15m, a maximum local distance of $d_l = 1m$, a velocity magnitude of $v_{magn} = 1 m.s^{-1}$, and an integration time step $\Delta T = 0.05s$. 

The robot model used for data annotation is the one described in Section~\ref{sec:unic_model}, with the following set of uncertain parameters $\p = [r, \, d]^T \in \mathbb{R}^{2}$.
The nominal values of the uncertain parameters used for the robot model are  $\p_n = [0.1m, \, 0.4m]^T$, with an associated uncertainty range of $\delta\p = [3\%, \, 3\%]^T$, representing the percentage deviation from their corresponding nominal values.
The controller gains are $\k_c = [1.5, \, 8.0, \, 0.2]^T$.

To demonstrate the reliability and generalizability of the learned model, a test set comprising 1.000 trajectories was generated using the same procedure, but with a velocity magnitude of $v_{magn} = 1.1 m.s^{-1}$ for the Dubins curve parametrization. 
Consequently, the test set includes trajectories with higher velocities than those in the validation set.

The mean and standard deviation values of the various components of the output vector for the validation and test sets generated by this setup are provided in Table~\ref{tab:datas_stats_unic}.

\subsubsection{Quadrotor}\label{sec:dataset_quad}

\begin{figure} [t]
    \centering
    \includegraphics[width=0.49\linewidth]{figures/learning_quadrotor/vnorm_val2.png}
    \includegraphics[width=0.49\linewidth]{figures/learning_quadrotor/vnorm_test.png}
    \caption{Velocity norm distribution in the training (left) and test sets (right) for the quadrotor.}%
    \label{fig: valvstest}%
\end{figure}

\begin{table}[t]
    \centering
    \begin{tabular}{ | c | c || c |}
    \hline
      \textbf{Output}  & \textbf{Validation set}  & \textbf{Test set} \\ \hline
    $\Rq$ & $1.0e^{-1} \pm 2.0e^{-2}$ & $1.1e^{-2} \pm 2.1e^{-2}$ \\ \hline
    $\u$ & $12469.3 \pm 861.6$ & $12476.8 \pm 1016.5$ \\ \hline
    $\Ru$ & $7782.6 \pm 3172.3$ & $7828.6 \pm 2845.7$ \\ \hline
\end{tabular}
\caption{
Mean and standard deviation of the output vector components norm after data annotation for the validation and test sets for the quadrotor case.
$\Rq$ is expressed in \emph{m}, and $(\u, \Ru)$, are squared propeller angular velocities [(rad/s)²].}
 \label{tab:datas_stats}
\end{table}

The local planner used to generate the quadrotor datasets is the kinosplines method described in Section~\ref{sec:kinosplines}, that enforced the following kinodynamic constraints on the generated splines $[v_{max}, a_{max}, j_{max}, s_{max}] = [5.0 \, m.s^{-1}, \allowbreak 1.5 \, m.s^{-2}, \allowbreak 15.0 \, m.s^{-3}, \allowbreak 30.0 \, m.s^{-4}]$. 
The trajectories of the dataset were generated considering a total distance $D_{max}$ of 15s, a maximum local distance of $d_l = 1s$ and an integration time step $\Delta T = 0.05s$. 

The quadrotor model used for data annotation is the one described in Section~\ref{sec:quad_model}, with the following set of uncertain parameters $\p$ = $[m, \, \allowbreak x_{cx}, \, \allowbreak x_{cy}, \, \allowbreak J_{x}, \, \allowbreak J_{y}, \,\allowbreak J_{z}]^T \in \mathbb{R}^{6}$, with $m$ the quadrotor mass, $x_{cx,y}$ a shift of the system center of mass along the \{x,y\}-axis, and $J_{x,y,z}$ the main inertia coefficients.
The nominal values of the uncertain parameters used for the robot model are $\p_{n}$ = $[1.113, \allowbreak0.0, \allowbreak0.0, \allowbreak0.015, \allowbreak0.015, \allowbreak0.007]^T$ and their associated uncertainty range used for the tubes computation are $\delta\p = [7\%, \allowbreak3cm, \allowbreak3cm, \allowbreak10\%, \allowbreak10\%, \allowbreak10\%]^T$, which represents the variation of the parameters w.r.t. their associated nominal value.
This choice of uncertain parameters is discussed in relation to the experimental validation presented in Chapter~\ref{chap:sampNN}.
The controller gains are $\boldsymbol{k}_{x} = [20.0, \, 20.0, \, 25.0]^T$, $\boldsymbol{k}_{v}= [9.0, \, 9.0, \, 12.0]^T$, $\boldsymbol{k}_{R}=[4.6, \, 4.6, \, 0.8]^T$, and $\boldsymbol{k}_{\omega}=[0.5, \, 0.5, \, 0.08]^T$.

In order to show the reliability and generalizability of the learned model, a test set composed of 1.000 trajectories was generated in the same way, but considering a maximum local duration $d_l = 2s$.
As a result, trajectories with higher velocities are encountered in the test set compared to the validation set, as depicted in Figure~\ref{fig: valvstest} where velocity norms can reach up to 7 m.s$^{-1}$ in the test set, compared with only 3 m.s$^{-1}$ in the validation set.
Note that the velocity norms exceed the velocity limit $v_{max}$, this is expected since this limit applies to the components of the velocity vector rather than the norm.

The mean and standard deviation values of the various components of the output vector for the validation and test sets generated by this setup are provided in Table~\ref{tab:datas_stats}.

\section{Evaluation}\label{sec:nn_eval}

In order to demonstrate the necessity of using a recurrent neural network due to the temporal dependencies of the predictions, a simple \textbf{MLP} baseline is implemented by substituting the recurrent layer with a single-layer linear encoder which takes as input a single element in the sequence and outputs its corresponding predictions.

Also, in order to justify the choice of a \textbf{GRU} architecture, the proposed model is compared to two other versions which replace the GRU block by a basic RNN \cite{cRNN} and an LSTM \cite{cLSTM} block respectively.
Note that an extensive comparison with Transformers \cite{cTrans} is not performed as they do not benefit from the hidden state when starting predictions from non zero values, and poorly generalize to longer sequences than those in the training set as it is briefly highlighted in Section~\ref{sec:nn_comparaison_unic}.

Finally, in order to measure the computational cost gain achieved by the proposed method and compare the inference time of the neural network to traditional methods, two known \myglsentry{odes} integrators are implemented for computing uncertainty tubes.
The first is \textbf{dopri5}, which leverages the Runge Kutta-4 algorithm to numerically approximate the solutions to \myglsentry{odes}. 
The second is the \textbf{Euler} method, which is known to be faster but yields less accurate results.

\subsection{Metrics}\label{sec:metric}

The performance of the model is evaluated using the Mean Absolute Error (MAE) over the different outputs. 
Rather than comparing the results on each sub-component of the input (e.g $\{x, y, z\}$ for $\Rq$ in the quadrotor case), they are combined into a single metric in order to obtain simpler and more general comparisons, as follows:
\begin{itemize}
    \item $\mathbf{MAE_{\Rq}}$ : represents the mean absolute error on the norm of $\Rq$. 
    For a given datapoint, given the ground truth and predicted state uncertainty tubes $\Rq$ and $\mathbf{\hat{r}_q}$, their respective norms $\|\Rq\|$ and $\|\mathbf{\hat{r}_q}\|$, are computed. 
    The mean absolute error, $\mathbf{MAE_{\Rq}}$, is then defined as:
    \begin{equation}\label{eq:MAE rq}
        \mathbf{MAE_{\Rq}} = MAE(\|\Rq\|, \|\mathbf{\hat{r}_q}\|)
    \end{equation}
    This metric is expressed in meters (m).
    \item $\mathbf{MAE_{u}}$ : represents the mean absolute error on the norm of $\u$. 
    The norms of the ground truth and predicted control inputs, $|\u|$ and $|\hat{\u}|$, are computed. 
    The mean absolute error, $\mathbf{MAE_{u}}$, is then calculated as the mean absolute error between these two norms. 
    It is expressed in [(rad/s)²] for the quadrotor case, and in [(rad/s)] for the differential drive robot one.
    \item $\mathbf{MAE_{\Ru}}$ : represents the mean absolute error on the norm of $\Ru$. 
    As for the two previous metrics, $\mathbf{MAE_{\Ru}}$ is defined as the mean absolute error between the norms $\|\Ru\|$ and $\|\mathbf{\hat{r}_u}\|$. 
    This metric is expressed in [(rad/s)²] for the quadrotor case, and in [(rad/s)] for the differential drive robot one.
\end{itemize}

These metrics are averaged over all elements of a sequence, then averaged over all sequences in the validation and test sets. 
In addition, the inference time is also used as metric in order to evaluate the computational cost of the method compared to the defined baselines, as well as traditional uncertainty tubes computation methods involving an ODE solver. 
Time is denoted $T_{solver/NN}$ according to the model or solver used.

\subsection{Training}

This section outlines the training procedures and specifies the neural network parameters used.

\subsubsection{Differential drive robot}

Before training the neural network, the input features of the dataset (train, val, and test) were min-max scaled according to the velocity magnitude value used when generating the dataset (see Section~\ref{sec:dataset_unic}) as it is guaranteed that the velocities components generated cannot exceed this value, ensuring that all velocity values are in the interval $[-v_{magn}, v_{magn}]$.  
The annotations were also normalized using a standard scaling based on the mean and standard deviation values of the training set annotations.

Finally, after data normalization, the neural networks were trained for 100 epochs using the Adam optimizer \cite{kingma2014adam} with an appropriate learning rate mentioned below and a batch size of 128. 
The \textbf{RNN},\textbf{LSTM} and \textbf{GRU} models were trained with a hidden state size of 128, a \textbf{MLP} composed of 3 linear layers, and a learning rate of $1e^{-3}$ for the \textbf{RNN}, the \textbf{LSTM} and \textbf{GRU} models.
Note that an LSTM layer is composed of a hidden state and a cell state (contrarily to GRUs and RNNs which have a hidden state only), so the real LSTM latent state size is 2$\times$64.
All used hyper-parameters were validated using a grid search.
They were trained using the MSE (Mean Squared Error) loss function and evaluated continuously on the validation set. 
The model weights achieving the best performance on the validation set were saved and used during the experiments.

\subsubsection{Quadrotor}\label{sec:train_quad}

Before training the neural network, the input features of the dataset (train, val, and test) were min-max scaled according to the maximum velocity and acceleration values used to generate the kinosplines (see Section~\ref{sec:dataset_quad}) as it is guaranteed that the velocities and accelerations generated cannot exceed these values. 
This ensures that all velocity values are in the interval $[-v_{max}, v_{max}]$, while all the acceleration values are in $[-a_{max}, a_{max}]$. 
The annotations were also normalized using a standard scaling based on the mean and standard deviation values of the training set annotations.

Finally, after data normalization, the neural networks were trained for 200 epochs using the Adam optimizer \cite{kingma2014adam} with an appropriate learning rate mentioned below and a batch size of 128. 
The \textbf{RNN},\textbf{LSTM} and \textbf{GRU} models were trained with a hidden state size of 512, a \textbf{MLP} composed of 3 linear layers, and a learning rate of $1e^{-4}$ for the \textbf{RNN} case against $1e^{-3}$ for both the \textbf{LSTM} and \textbf{GRU} models.
Note that an LSTM layer is composed of a hidden state and a cell state (contrarily to GRUs and RNNs which have a hidden state only), so the real LSTM latent state size is 2$\times$512.
All used hyper-parameters were validated using a grid search.
They were trained using the MSE (Mean Squared Error) loss function and evaluated continuously on the validation set. 
The model weights achieving the best performance on the validation set were saved and used during the experiments.

\subsection{Implementation details}

The ODEs were implemented using the JiTCODE \cite{cJit} module which converts the equations to be integrated into C-compiled code.
The \textbf{Euler} method or the \textbf{dopri5} integrator were then used to solve each ODE and take advantage of this compiled function.

All following results were obtained on an Intel i9 CPU@2.6GHz processor with one RTX A3000 GPU. 

\section{Simulation results}\label{sec:nn_results}

\subsection{Differential Drive Robot}\label{sec:nn_results_unic}

\subsubsection{Model comparison}\label{sec:nn_comparaison_unic}

\input{figures/tables/NN_unic_model_comp.tex}

\subsubsection{Qualitative results}

\subsection{Quadrotor}\label{sec:nn_results_quad}

\subsubsection{Model comparison}\label{sec:nn_comparaison_quad}

\input{figures/tables/NN_quad_model_comp.tex}

Table~\ref{tab:NN_results_table_Q} reports the MAE for the different output vector components on the validation and test sets. 
First of all, note that the \textbf{MLP} provides a poor performance, confirming the need for a recurrent neural network.
A deviation of up to 30\% from the average expected values (cf. Table~\ref{tab:datas_stats}) is observed in the validation and test sets.

Among recurrent models, \textbf{RNN} offers the least accurate predictions, with up to 7\% error on the $\boldsymbol{r_u}$ components of the test set.
On the other hand, \textbf{GRU} provides the best accuracy on all the components on both sets except for $\Ru$ on the validation set, but for which predictions remain close to expected values with less than 1\% deviation from expected mean values.
\textbf{GRU} shows the best reliability and generalizability of the trained model to unseen samples from a different distribution.
\textbf{GRU} performance on the test set shows that the predictions along $\boldsymbol{r_q}$ remain highly accurate (less than 1 millimeter average error) and that the highest errors are obtained on $\boldsymbol{r_u}$ but do not exceed 4\% of the expected average value.
The \textbf{LSTM} provides the best predictions on the $\Ru$ component of the validation set. 
However, the results show a slightly lower accuracy than \textbf{GRU} on the other components with an average error of 4.5\% on test set $\boldsymbol{r_u}$ predictions.
Nevertheless, \textbf{LSTM} is more accurate on all components than \textbf{RNN}.
Overall, \textbf{RNN} performs the worst while \textbf{GRU} and \textbf{LSTM} provide similar results, with an overall better accuracy for \textbf{GRU}.
Moreover, the latter shows a better generalization to unseen trajectories that are slightly different from the training set.

In order to choose the most advantageous model or method for computing uncertainty tubes in terms of prediction time, the methods/models are applied to trajectories of different lengths (i.e. made up of a certain number of desired states). 
Table~\ref{tab:timepred} shows the results obtained on trajectories of several hundred states. 
Note that with the current system, the number of ordinary equations solved for each element in the sequence (i.e. trajectory state) is equal to 91 (see Sect.~\ref{sec: sensi}).
We observe that the greater the length of the trajectory to be integrated, the greater the gap between ODEs solver methods and neural networks in average prediction time.
Results show that as the number of states in the trajectory increases, the time gain reaches two orders of magnitude using recurrent neural network architectures.
We also note that among these models, \textbf{RNN} is the fastest to perform the predictions, which is explained by the fact that the network size is smaller than \textbf{LSTM}, and the \textbf{RNN} cell performs fewer internal operations than the \textbf{GRU} cell.
In a robust sampling-based motion planning context, the neural network is meant to be queried tens of thousands of times on multi-states trajectories.
Hence, these observed gains can be exponential depending on the number of trajectories to be evaluated.

Finally, even if \textbf{RNN} excels in inference time when making predictions; its accuracy is noticeably lower when contrasted with that of \textbf{GRU}.
As for the \textbf{LSTM}, it provides the slowest inference time and less accurate predictions than the \textbf{GRU}, except for the $\Ru$ component of the validation set.
Additionally, its hidden state implementation results in twice as many parameters having to be saved per sampling-based motion planner iteration compared to \textbf{GRU}, which translates by a higher memory cost when growing trees or graphs with thousands of nodes.
Therefore, based on the models implementation details and results presented above, the use of \textbf{GRU} is recommended in the context of a sampling-based motion planner.
Indeed, the latter is much faster than methods based on ODEs solvers while offering the most accurate predictions, thus offering the best trade-off between inference time and accuracy.

\subsubsection{Ablation study}

\input{figures/tables/NN_quad_ablation.tex}

In theory, all the inputs to our neural network are needed by the controller and the uncertainty tubes computation methods to obtain the desired outputs. 
In order to confirm this for the neural network as well, an ablation study is conducted on the \textbf{GRU} model to show that the choice of input vector components described in Section~\ref{sec:quad_nn_architecture} is the most relevant for the quadrotor case. 
The components considered for the study are the desired linear and angular velocities, and the desired accelerations, denoted $V$ and $A$ respectively. 
The proposed model takes both inputs and is denoted \textbf{GRU$\boldsymbol{_{VA}}$}. 
Two ablations are defined, one for each input, resulting in two models: \textbf{GRU$\boldsymbol{_V}$} which takes as input the linear and angular velocities only, and \textbf{GRU$\boldsymbol{_A}$} which only takes the accelerations as input. 
Each model is trained in the same way detailed in Section~\ref{sec:train_quad} and is evaluated according to the MAE metrics described in Section~\ref{sec:metric}.

Note that since the network is intended to be used in a sampling-based approach, no ablation study is performed on the output vector since our objective is to use a single model within the planner, keeping the memory and computational costs low.

Table~\ref{tab:ablation} provides the results of the ablation study.
Results show that each input component specializes in predicting one of the desired outputs.
Specifically, the model \textbf{GRU$\boldsymbol{_V}$} using the velocity component only yields accurate results for $\Rq$ and $\Ru$ on both the validation and test sets, while the one solely using the accelerations leads to accurate results on $\u$ on both sets.

These outcomes are expected since $\Rq$ depends indirectly on the state of the robot $\q$ only, which can be inferred from the velocities easily. 
On the other hand, the inner workings of the chosen controller give more importance to the accelerations of the robot, while the velocities are used for computing internal errors. T
his explains the fact that \textbf{GRU$\boldsymbol{_A}$} yields good results for the $\u$ predictions.

On the other hand, Table~\ref{tab:ablation} shows an improvement on both the control inputs $\u$ and the control uncertainty tubes $\Ru$ using both the velocities and the accelerations as inputs to the neural network. 
Indeed, even though a slight degradation can be noted in the performance of $\Rq$ predictions, \textbf{GRU$\boldsymbol{_{VA}}$} achieves the best overall results on both the validation and test sets, thus justifying the implementation of Section~\ref{sec:architecture}. 
This can be explained by the fact that the output values are not related to the input components independently, they also depend on the interactions between these variables. 
Also, remind that the chosen controller and uncertainty tubes computation methods use both the velocities and the accelerations of the robot to obtain the desired outputs.

Note that the results of this ablation study are specific to the chosen case of the quadrotor/controller.
Another dynamical system might need a different set of inputs to obtain accurate predictions.

\subsubsection{Qualitative results}

\begin{figure}[b]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/learning_quadrotor/long_pred.png}
    \caption{Example of \textbf{GRU} predictions on a 600-state trajectory generated in the same way as the test set. $||\Rq||, ||\u||$ and $||\Ru||$ refer to the norm of their respective vector. Predicted outputs are displayed in blue against true values in black. $||\Rq||$ is expressed in $m$, and control input associated values ($||\u||,||\Ru||)$ are squared propeller angular velocities [(rad/s)²].}
    \label{fig:long_test}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.49\linewidth}
         \centering
    \includegraphics[width=\linewidth]{figures/learning_quadrotor/pred_u.png}
    \includegraphics[width=\linewidth]{figures/learning_quadrotor/pred_rx.png}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
         \centering
    \includegraphics[width=\linewidth]{figures/learning_quadrotor/pred_ru.png}
    \includegraphics[width=\linewidth]{figures/learning_quadrotor/pred_ry.png}
    \end{subfigure}
    \includegraphics[width=0.49\linewidth]{figures/learning_quadrotor/pred_rz.png}
    \caption{Example of \textbf{GRU} predictions on a 300-state trajectory of the test set. 
    Predicted outputs are displayed in blue against true values in black. $r_{x}, r_{y}, r_{z}$ are expressed in $m$, and control input associated values ($u_{i},r_{ui}$) are squared propeller angular velocities [(rad/s)²].}
    \label{fig:pred_test}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/learning_quadrotor/all_models.png}
    \caption{Comparison of predictions obtained using the different recurrent neural network architectures as well as the \textbf{MLP} on a 300-state trajectory of the test set. $||\Rq||, ||\u||$ and $||\Ru||$ refer to the norm of their respective vector. 
    True values are displayed in black, \textbf{GRU} predictions in blue, \textbf{MLP} predictions in purple, \textbf{LSTM} outputs in yellow, and basic \textbf{RNN} predictions are in red.}
    \label{fig:MLP_pred_val}
\end{figure}

The robust generalization of the \textbf{GRU} model to the test set is highlighted in Figure~\ref{fig:pred_test}, depicting predictions for all the components of the output vector for a trajectory composed of 300 states (i.e. $T_F=15s$).
One can observe larger prediction errors during transient phases involving the higher velocities present in the test set than in the training one.
Note that the neural network predictions in these phases have a moving average `behavior'. 
However, an overall good fitting of the predictions can be highlighted.

In addition to the results discussed in Section~\ref{sec:nn_comparaison_quad}, Figure~\ref{fig:MLP_pred_val} shows an example of \textbf{MLP}'s inability to provide accurate predictions on a 300-state test set trajectory, with a tendency to simply average expected values over the latter.
In comparison, the \textbf{GRU} model is able to accurately predict the control inputs and the uncertainty tube across the whole trajectory. 
Also note a close performance between \textbf{GRU} and \textbf{LSTM} on all tasks, while \textbf{RNN} lags behind and tends to underestimate large variations of control uncertainty tubes $\Ru$.

Finally, in order to show the stability of the proposed method, Figure~\ref{fig:long_test} depicts the norm of predicted vectors for a 600-state trajectory generated in the same way as the test set but considering a total length of 30s (i.e. $T_F=30s$).
Note that this trajectory is twice as long as those used in training, validation and test sets.
The results show the same behavior in transient phases where higher velocities are encountered.
However, one can observe that the model is consistent throughout longer sequences even with a different distribution from the one it was trained one. 
This is convenient in a sampling-based motion planning context, where successive local trajectories must be evaluated.
Note that, in the dataset all trajectories start from a hovering state (i.e., null velocities and accelerations). 
Hence, in order to generalize to trajectories where initial velocities and accelerations are not null one can simply append a planned sub-trajectory arriving at this initial state and starting from the hovering state.

\section{Conclusion} \label{sec:NN_concl}
